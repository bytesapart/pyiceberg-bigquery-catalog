"""
BigQuery catalog implementation for PyIceberg.
"""

from typing import Any, Dict, List, Literal, Optional, Set, Tuple, Union
import json
import re
import uuid
from datetime import datetime

from google.cloud import bigquery
from google.cloud.exceptions import Conflict, NotFound
from google.api_core.exceptions import PreconditionFailed

from pyiceberg.catalog import MetastoreCatalog, PropertiesUpdateSummary, CatalogType
from pyiceberg.exceptions import (
    CommitFailedException,
    NamespaceAlreadyExistsError,
    NamespaceNotEmptyError,
    NoSuchNamespaceError,
    NoSuchPropertyException,
    NoSuchTableError,
    TableAlreadyExistsError,
    ValidationError,
)
from pyiceberg.io import FileIO
from pyiceberg.partitioning import UNPARTITIONED_PARTITION_SPEC, PartitionSpec
from pyiceberg.schema import Schema
from pyiceberg.serializers import FromInputFile, ToOutputFile
from pyiceberg.table import CommitTableRequest, CommitTableResponse, Table
from pyiceberg.table.metadata import TableMetadata, new_table_metadata
from pyiceberg.table.sorting import UNSORTED_SORT_ORDER, SortOrder
from pyiceberg.typedef import EMPTY_DICT, Identifier, Properties


class BigQueryCatalog(MetastoreCatalog):
    """
    BigQuery catalog implementation for Apache Iceberg.

    This catalog uses BigQuery's external table features to store Iceberg metadata.
    It can be configured using the py-catalog-impl approach:

    Example:
        catalog:
            bigquery:
                py-catalog-impl: pyiceberg_bigquery_catalog.BigQueryCatalog
                project_id: my-project
                dataset_id: my_dataset
                gcp_location: us-central1
                warehouse: gs://my-bucket/warehouse
    """

    # Configuration keys
    PROPERTIES_KEY_PROJECT_ID = "project_id"
    PROPERTIES_KEY_DATASET_ID = "dataset_id"
    PROPERTIES_KEY_GCP_LOCATION = "gcp_location"
    PROPERTIES_KEY_FILTER_UNSUPPORTED_TABLES = "filter_unsupported_tables"
    PROPERTIES_KEY_BQ_CONNECTION = "bq_connection"

    # External table metadata keys
    EXTERNAL_TABLE_TYPE_KEY = "table_type"
    EXTERNAL_TABLE_TYPE_VALUE = "iceberg"
    EXTERNAL_METADATA_LOCATION_KEY = "metadata_location"
    EXTERNAL_PREVIOUS_METADATA_LOCATION_KEY = "previous_metadata_location"
    EXTERNAL_LOCATION_KEY = "location"
    EXTERNAL_UUID_KEY = "uuid"

    def __init__(self, name: str, **properties: Any):
        """
        Initialize the BigQuery catalog.

        Args:
            name: The catalog name
            **properties: Catalog configuration properties

        Raises:
            NoSuchPropertyException: If required properties are missing
        """
        super().__init__(name, **properties)

        # Validate required properties
        self.project_id = properties.get(self.PROPERTIES_KEY_PROJECT_ID)
        if not self.project_id:
            raise NoSuchPropertyException(
                f"Property '{self.PROPERTIES_KEY_PROJECT_ID}' is required for BigQuery catalog"
            )

        self.dataset_id = properties.get(self.PROPERTIES_KEY_DATASET_ID)
        if not self.dataset_id:
            raise NoSuchPropertyException(
                f"Property '{self.PROPERTIES_KEY_DATASET_ID}' is required for BigQuery catalog"
            )

        # Optional properties
        self.gcp_location = properties.get(self.PROPERTIES_KEY_GCP_LOCATION, "us")
        self.filter_unsupported_tables = (
            str(properties.get(self.PROPERTIES_KEY_FILTER_UNSUPPORTED_TABLES, "false")).lower() == "true"
        )
        self.warehouse_location = properties.get("warehouse")

        # Initialize BigQuery client
        self.client = bigquery.Client(project=self.project_id)

        # File IO for metadata operations
        self._file_io: Optional[FileIO] = None

        # Create the dataset if it doesn't exist
        self._ensure_dataset_exists()

    @property
    def file_io(self) -> FileIO:
        """Get or create FileIO instance."""
        if self._file_io is None:
            self._file_io = self._load_file_io(self.properties)
        return self._file_io

    def create_table(
            self,
            identifier: Union[str, Identifier],
            schema: Union[Schema, "pa.Schema"],
            location: Optional[str] = None,
            partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,
            sort_order: SortOrder = UNSORTED_SORT_ORDER,
            properties: Properties = EMPTY_DICT,
    ) -> Table:
        """Create a table in BigQuery."""
        schema = self._convert_schema_if_needed(schema)
        table_name = self._validate_identifier(identifier)
        table_identifier = self._full_identifier(table_name)

        # Check if table already exists
        if self.table_exists(identifier):
            raise TableAlreadyExistsError(f"Table already exists: {'.'.join(table_identifier)}")

        # Determine table location
        location = self._resolve_table_location(location, self.dataset_id, table_name)

        # Create table metadata
        metadata = new_table_metadata(
            location=location,
            schema=schema,
            partition_spec=partition_spec,
            sort_order=sort_order,
            properties=properties,
        )

        # Write metadata file - the _get_metadata_location method needs both location and version
        metadata_location = self._get_metadata_location(location, 0)  # New tables start at version 0
        ToOutputFile.table_metadata(metadata, self.file_io.new_output(metadata_location))

        # Note: We're NOT creating the BigQuery external table yet
        # It will be created during the first commit (similar to Java implementation)
        print(f"Created Iceberg metadata for table {table_name}")
        print(f"BigQuery external table will be created after first data write")

        return self._create_table_instance(
            identifier=table_identifier,
            metadata=metadata,
            metadata_location=metadata_location,
        )

    def load_table(self, identifier: Union[str, Identifier]) -> Table:
        """Load a table from BigQuery."""
        table_name = self._validate_identifier(identifier)
        table_identifier = self._full_identifier(table_name)

        # Get BigQuery table
        bq_table_ref = bigquery.TableReference(
            bigquery.DatasetReference(self.project_id, self.dataset_id),
            table_name
        )

        try:
            bq_table = self.client.get_table(bq_table_ref)
        except NotFound:
            raise NoSuchTableError(f"Table does not exist: {'.'.join(table_identifier)}")

        # Extract metadata location from external table configuration
        metadata_location = self._get_metadata_location_from_table(bq_table)
        if not metadata_location:
            raise NoSuchTableError(
                f"Table {'.'.join(table_identifier)} is not a valid Iceberg table"
            )

        # Load metadata
        metadata_file = self.file_io.new_input(metadata_location)
        metadata = FromInputFile.table_metadata(metadata_file)

        return self._create_table_instance(
            identifier=table_identifier,
            metadata=metadata,
            metadata_location=metadata_location,
        )

    def table_exists(self, identifier: Union[str, Identifier]) -> bool:
        """Check if a table exists."""
        try:
            self.load_table(identifier)
            return True
        except NoSuchTableError:
            return False

    def drop_table(self, identifier: Union[str, Identifier]) -> None:
        """Drop a table from BigQuery."""
        table_name = self._validate_identifier(identifier)
        bq_table_ref = bigquery.TableReference(
            bigquery.DatasetReference(self.project_id, self.dataset_id),
            table_name
        )

        try:
            self.client.delete_table(bq_table_ref)
        except NotFound:
            table_identifier = self._full_identifier(table_name)
            raise NoSuchTableError(f"Table does not exist: {'.'.join(table_identifier)}")

    def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -> Table:
        """Rename a table - not supported in BigQuery."""
        raise ValidationError("Table rename operation is not supported in BigQuery catalog")

    def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -> None:
        """Create a namespace (dataset) in BigQuery - not supported as dataset is pre-configured."""
        raise ValidationError(
            "BigQuery catalog is configured with a fixed dataset. "
            "Namespace creation is not supported. "
            f"Using dataset: {self.dataset_id}"
        )

    def drop_namespace(self, namespace: Union[str, Identifier]) -> None:
        """Drop a namespace (dataset) from BigQuery - not supported as dataset is pre-configured."""
        raise ValidationError(
            "BigQuery catalog is configured with a fixed dataset. "
            "Namespace deletion is not supported. "
            f"Using dataset: {self.dataset_id}"
        )

    def list_tables(self, namespace: Union[str, Identifier] = ()) -> List[Identifier]:
        """List tables in the configured dataset."""
        # Always use the configured dataset
        try:
            tables = []
            for table in self.client.list_tables(f"{self.project_id}.{self.dataset_id}"):
                if self.filter_unsupported_tables:
                    try:
                        bq_table = self.client.get_table(table.reference)
                        if self._is_iceberg_table(bq_table):
                            tables.append((self.dataset_id, table.table_id))
                    except:
                        continue
                else:
                    tables.append((self.dataset_id, table.table_id))
            return tables
        except NotFound:
            return []

    def list_namespaces(self, namespace: Union[str, Identifier] = ()) -> List[Identifier]:
        """List namespaces - returns only the configured dataset."""
        # Only return the configured dataset
        return [(self.dataset_id,)]

    def load_namespace_properties(self, namespace: Union[str, Identifier]) -> Properties:
        """Load properties for a namespace."""
        # Validate it's requesting the configured dataset
        namespace_tuple = self.identifier_to_tuple(namespace)
        if len(namespace_tuple) != 1 or namespace_tuple[0] != self.dataset_id:
            raise NoSuchNamespaceError(f"Namespace does not exist: {namespace_tuple}")

        dataset_ref = bigquery.DatasetReference(self.project_id, self.dataset_id)

        try:
            dataset = self.client.get_dataset(dataset_ref)
            properties: Properties = {}

            # Convert labels back to properties
            if dataset.labels:
                properties.update(self._labels_to_properties(dataset.labels))

            # Add location property
            if dataset.location:
                properties["location"] = dataset.location

            return properties
        except NotFound:
            raise NoSuchNamespaceError(f"Namespace does not exist: {namespace_tuple}")

    def update_namespace_properties(
        self,
        namespace: Union[str, Identifier],
        removals: Optional[Set[str]] = None,
        updates: Properties = EMPTY_DICT,
    ) -> PropertiesUpdateSummary:
        """Update properties for a namespace."""
        # Validate it's the configured dataset
        namespace_tuple = self.identifier_to_tuple(namespace)
        if len(namespace_tuple) != 1 or namespace_tuple[0] != self.dataset_id:
            raise NoSuchNamespaceError(f"Namespace does not exist: {namespace_tuple}")

        dataset_ref = bigquery.DatasetReference(self.project_id, self.dataset_id)

        try:
            dataset = self.client.get_dataset(dataset_ref)
            current_labels = dataset.labels or {}

            # Convert current labels to properties
            current_properties = self._labels_to_properties(current_labels)

            # Apply updates and removals
            summary, updated_properties = self._get_updated_props_and_update_summary(
                current_properties, removals, updates
            )

            # Convert properties back to labels
            dataset.labels = self._properties_to_labels(updated_properties)

            # Update the dataset
            self.client.update_dataset(dataset, ["labels"])

            return summary
        except NotFound:
            raise NoSuchNamespaceError(f"Namespace does not exist: {namespace_tuple}")

    def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -> Table:
        """Register an existing Iceberg table in BigQuery."""
        table_name = self._validate_identifier(identifier)
        table_identifier = self._full_identifier(table_name)

        # Check if table already exists
        if self.table_exists(identifier):
            raise TableAlreadyExistsError(f"Table already exists: {'.'.join(table_identifier)}")

        # Load metadata from the provided location
        metadata_file = self.file_io.new_input(metadata_location)
        metadata = FromInputFile.table_metadata(metadata_file)

        # Create BigQuery external table
        bq_table_ref = bigquery.TableReference(
            bigquery.DatasetReference(self.project_id, self.dataset_id),
            table_name
        )
        bq_table = self._create_bigquery_external_table(
            table_ref=bq_table_ref,
            metadata=metadata,
            metadata_location=metadata_location,
        )

        try:
            self.client.create_table(bq_table)
        except Conflict:
            raise TableAlreadyExistsError(f"Table already exists: {'.'.join(table_identifier)}")

        return self._create_table_instance(
            identifier=table_identifier,
            metadata=metadata,
            metadata_location=metadata_location,
        )

    def commit_table(self, table_request: CommitTableRequest) -> CommitTableResponse:
        """Update one or more tables.

        This is now a public method in PyIceberg 0.9.1, renamed from _commit_table.
        """
        identifier = table_request.identifier
        table_name = identifier.name

        # Load current table state
        current_table = None
        bq_table_exists = False
        try:
            # Try to get the BigQuery table
            bq_table_ref = bigquery.TableReference(
                bigquery.DatasetReference(self.project_id, self.dataset_id),
                table_name
            )
            self.client.get_table(bq_table_ref)
            bq_table_exists = True
            current_table = self.load_table(table_name)
        except NotFound:
            # BigQuery table doesn't exist yet
            bq_table_exists = False
            # Try to load just the metadata
            try:
                # Check if metadata exists
                metadata_files = list(
                    self.file_io.list(f"{self._resolve_table_location(None, self.dataset_id, table_name)}/metadata/"))
                if metadata_files:
                    # Load the latest metadata file
                    latest_metadata = sorted(metadata_files)[-1]
                    metadata_file = self.file_io.new_input(latest_metadata)
                    current_metadata = FromInputFile.table_metadata(metadata_file)
                    current_table = self._create_table_instance(
                        identifier=(self.name, self.dataset_id, table_name),
                        metadata=current_metadata,
                        metadata_location=latest_metadata,
                    )
            except:
                current_table = None
        except NoSuchTableError:
            current_table = None

        # Stage the updates
        staged_table = self._update_and_stage_table(current_table, table_request)

        # Write new metadata
        ToOutputFile.table_metadata(staged_table.metadata, self.file_io.new_output(staged_table.metadata_location))

        # Update BigQuery table
        bq_table_ref = bigquery.TableReference(
            bigquery.DatasetReference(self.project_id, self.dataset_id),
            table_name
        )

        if not bq_table_exists and staged_table.metadata.current_snapshot():
            # Create new table in BigQuery now that we have a snapshot
            bq_table = self._create_bigquery_external_table(
                table_ref=bq_table_ref,
                metadata=staged_table.metadata,
                metadata_location=staged_table.metadata_location,
            )
            try:
                self.client.create_table(bq_table)
            except Exception as e:
                self.file_io.delete(staged_table.metadata_location)
                raise CommitFailedException(f"Failed to create BigQuery table: {str(e)}")
        elif bq_table_exists:
            # Update existing table
            try:
                bq_table = self.client.get_table(bq_table_ref)

                # Update external table metadata
                self._update_bigquery_table_metadata(
                    bq_table,
                    staged_table.metadata,
                    staged_table.metadata_location,
                )

                # Update the table
                self.client.update_table(bq_table, ["external_data_configuration"])

            except PreconditionFailed:
                self.file_io.delete(staged_table.metadata_location)
                raise CommitFailedException("Concurrent update detected")
            except Exception as e:
                self.file_io.delete(staged_table.metadata_location)
                raise CommitFailedException(str(e))

        return CommitTableResponse(
            metadata=staged_table.metadata,
            metadata_location=staged_table.metadata_location,
        )

    def list_views(self, namespace: Union[str, Identifier] = ()) -> List[Identifier]:
        """List views in the given namespace.

        BigQuery doesn't support Iceberg views, so return empty list.
        """
        return []

    def view_exists(self, identifier: Union[str, Identifier]) -> bool:
        """Check if a view exists.

        BigQuery doesn't support Iceberg views, so always return False.
        """
        return False

    def drop_view(self, identifier: Union[str, Identifier]) -> None:
        """Drop a view.

        BigQuery doesn't support Iceberg views, so raise an error.
        """
        raise ValidationError("BigQuery catalog does not support Iceberg views")

    def _ensure_dataset_exists(self) -> None:
        """Ensure the configured dataset exists in BigQuery."""
        dataset_ref = bigquery.DatasetReference(self.project_id, self.dataset_id)

        try:
            self.client.get_dataset(dataset_ref)
        except NotFound:
            # Create the dataset if it doesn't exist
            dataset = bigquery.Dataset(dataset_ref)
            dataset.location = self.gcp_location

            # Set default storage location if warehouse is configured
            if self.warehouse_location:
                default_location = f"{self.warehouse_location.rstrip('/')}/{self.dataset_id}.db"
                dataset.labels = {"default_storage_location": self._sanitize_label_value(default_location)}

            try:
                self.client.create_dataset(dataset)
            except Conflict:
                # Another process might have created it simultaneously
                pass  # pyiceberg_bigquery_catalog/catalog.py

    def _validate_identifier(self, identifier: Union[str, Identifier]) -> str:
        """Validate identifier and return table name only."""
        if isinstance(identifier, str):
            parts = identifier.split(".")
            if len(parts) == 1:
                # Just table name
                return parts[0]
            elif len(parts) == 2:
                # dataset.table format - validate dataset matches
                dataset, table = parts
                if dataset != self.dataset_id:
                    raise ValidationError(
                        f"Dataset '{dataset}' does not match configured dataset '{self.dataset_id}'"
                    )
                return table
            else:
                raise ValidationError(f"Invalid identifier format: {identifier}")
        else:
            # Tuple identifier
            if len(identifier) == 1:
                return identifier[0]
            elif len(identifier) == 2:
                dataset, table = identifier
                if dataset != self.dataset_id:
                    raise ValidationError(
                        f"Dataset '{dataset}' does not match configured dataset '{self.dataset_id}'"
                    )
                return table
            else:
                raise ValidationError(f"Invalid identifier format: {identifier}")

    def _full_identifier(self, table_name: str) -> Identifier:
        """Get the full identifier including catalog name."""
        return (self.name, self.dataset_id, table_name)

    def _resolve_table_location(self, location: Optional[str], dataset_name: str, table_name: str) -> str:
        """Resolve table location."""
        if location:
            return location.rstrip("/")

        # Use warehouse location if configured
        if self.warehouse_location:
            return f"{self.warehouse_location.rstrip('/')}/{dataset_name}.db/{table_name}"

        # Use dataset default storage location
        dataset_ref = bigquery.DatasetReference(self.project_id, self.dataset_id)
        dataset = self.client.get_dataset(dataset_ref)

        if dataset.labels and "default_storage_location" in dataset.labels:
            base_location = dataset.labels["default_storage_location"]
            return f"{base_location}/{table_name}"

        raise ValueError("No default path is set, please specify a location when creating a table")

    def _create_bigquery_external_table(
            self,
            table_ref: bigquery.TableReference,
            metadata: TableMetadata,
            metadata_location: str,
    ) -> bigquery.Table:
        """Create a BigQuery external table configuration for Iceberg."""
        table = bigquery.Table(table_ref)

        # Create external configuration
        external_config = bigquery.ExternalConfig("ICEBERG")
        # For Iceberg tables, BigQuery needs the metadata location as the source URI
        external_config.source_uris = [metadata_location]
        external_config.metadata_cache_mode = "MANUAL"

        # Build table metadata
        table_metadata = {
            self.EXTERNAL_TABLE_TYPE_KEY: self.EXTERNAL_TABLE_TYPE_VALUE,
            self.EXTERNAL_METADATA_LOCATION_KEY: metadata_location,
            self.EXTERNAL_LOCATION_KEY: metadata.location,
        }

        # Add properties
        if metadata.properties:
            table_metadata.update(metadata.properties)

        # Add UUID
        if metadata.table_uuid:
            table_metadata[self.EXTERNAL_UUID_KEY] = str(metadata.table_uuid)

        # Set metadata as JSON
        external_config.metadata = json.dumps(table_metadata)

        # Check for BigQuery connection
        if self.PROPERTIES_KEY_BQ_CONNECTION in metadata.properties:
            external_config.connection_id = metadata.properties[self.PROPERTIES_KEY_BQ_CONNECTION]

        table.external_data_configuration = external_config
        return table

    def _update_bigquery_table_metadata(
        self,
        bq_table: bigquery.Table,
        metadata: TableMetadata,
        metadata_location: str,
    ) -> None:
        """Update BigQuery external table metadata."""
        external_config = bq_table.external_data_configuration
        if not external_config:
            raise ValueError("Table is not an external table")

        try:
            table_metadata = json.loads(external_config.metadata or "{}")
        except json.JSONDecodeError:
            table_metadata = {}

        # Update metadata
        old_metadata_location = table_metadata.get(self.EXTERNAL_METADATA_LOCATION_KEY)
        if old_metadata_location:
            table_metadata[self.EXTERNAL_PREVIOUS_METADATA_LOCATION_KEY] = old_metadata_location

        table_metadata[self.EXTERNAL_METADATA_LOCATION_KEY] = metadata_location
        table_metadata[self.EXTERNAL_LOCATION_KEY] = metadata.location

        # Update properties
        if metadata.properties:
            table_metadata.update(metadata.properties)

        # Update UUID
        if metadata.table_uuid:
            table_metadata[self.EXTERNAL_UUID_KEY] = str(metadata.table_uuid)

        # Update snapshot statistics
        if metadata.current_snapshot():
            snapshot = metadata.current_snapshot()
            if snapshot.summary:
                summary = snapshot.summary
                if "total-data-files" in summary:
                    table_metadata["numFiles"] = summary["total-data-files"]
                if "total-records" in summary:
                    table_metadata["numRows"] = summary["total-records"]
                if "total-files-size" in summary:
                    table_metadata["totalSize"] = summary["total-files-size"]

        external_config.metadata = json.dumps(table_metadata)

    def _get_metadata_location_from_table(self, bq_table: bigquery.Table) -> Optional[str]:
        """Extract metadata location from BigQuery table."""
        if not bq_table.external_data_configuration:
            return None

        try:
            metadata = json.loads(bq_table.external_data_configuration.metadata or "{}")
            return metadata.get(self.EXTERNAL_METADATA_LOCATION_KEY)
        except json.JSONDecodeError:
            return None

    @staticmethod
    def _get_metadata_location(location: str, new_version: int = 0) -> str:
        """Generate metadata file location."""
        if new_version < 0:
            raise ValueError(f"Table metadata version: `{new_version}` must be a non-negative integer")
        version_str = f"{new_version:05d}"
        return f"{location}/metadata/{version_str}-{uuid.uuid4()}.metadata.json"

    def _is_iceberg_table(self, bq_table: bigquery.Table) -> bool:
        """Check if a BigQuery table is an Iceberg table."""
        if not bq_table.external_data_configuration:
            return False

        try:
            metadata = json.loads(bq_table.external_data_configuration.metadata or "{}")
            return (
                metadata.get(self.EXTERNAL_TABLE_TYPE_KEY, "").lower() == self.EXTERNAL_TABLE_TYPE_VALUE and
                self.EXTERNAL_METADATA_LOCATION_KEY in metadata
            )
        except json.JSONDecodeError:
            return False

    def _properties_to_labels(self, properties: Properties) -> Dict[str, str]:
        """Convert properties to BigQuery labels."""
        labels = {}
        for key, value in properties.items():
            # BigQuery labels have restrictions
            label_key = re.sub(r'[^a-z0-9_-]', '_', key.lower())[:63]
            label_value = self._sanitize_label_value(str(value))
            if label_key and label_value:
                labels[label_key] = label_value
        return labels

    @staticmethod
    def _labels_to_properties(labels: Dict[str, str]) -> Properties:
        """Convert BigQuery labels back to properties."""
        # Best effort conversion as original format is lost
        properties = {}
        for key, value in labels.items():
            # Try to restore original format
            prop_key = key.replace('_', '.')
            properties[prop_key] = value
        return properties

    @staticmethod
    def _sanitize_label_value(value: str) -> str:
        """Sanitize a value to be valid as a BigQuery label value."""
        if not value:
            return ""
        # BigQuery label values can only contain lowercase letters, numbers, hyphens, underscores
        sanitized = re.sub(r'[^a-z0-9_-]', '_', value.lower())[:63]
        return sanitized

    def _create_table_instance(
        self,
        identifier: Identifier,
        metadata: TableMetadata,
        metadata_location: str,
    ) -> Table:
        """Create a Table instance."""
        from pyiceberg.table import Table as IcebergTable

        return IcebergTable(
            identifier=identifier,
            metadata=metadata,
            metadata_location=metadata_location,
            io=self.file_io,
            catalog=self,
        )